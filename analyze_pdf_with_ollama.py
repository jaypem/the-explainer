import sys
import subprocess
from pathlib import Path
import PyPDF2


def get_pdf_text(pdf_path):
    try:
        print(f"ğŸ“– Ã–ffne PDF-Datei: {pdf_path.name}")
        with open(pdf_path, "rb") as file:
            reader = PyPDF2.PdfReader(file)
            total_pages = len(reader.pages)
            print(f"ğŸ“„ Gefunden: {total_pages} Seite(n)")

            text_parts = []
            for i, page in enumerate(reader.pages, 1):
                print(f"ğŸ“ Verarbeite Seite {i}/{total_pages}...", end="", flush=True)
                page_text = page.extract_text()
                if page_text:
                    text_parts.append(page_text)
                    print(f" âœ… ({len(page_text)} Zeichen)")
                else:
                    print(" âš ï¸ (leer)")

            full_text = "\n".join(text_parts)
            print(f"ğŸ“Š Gesamt extrahierte Zeichen: {len(full_text)}")
            return full_text.strip()
    except Exception as e:
        print(f"âŒ Fehler beim Auslesen des PDFs: {e}")
        sys.exit(1)


def build_prompt(role, document_text, language="de"):
    """Erzeugt den Prompt fÃ¼r das LLM.

    Parameters
    ----------
    role : str
        Expertenrolle (z.B. Jurist, Arzt, Steuerberater)
    document_text : str
        VollstÃ¤ndiger extrahierter Dokumententext
    language : str, optional
        ISO Sprachcode fÃ¼r die Ausgabe (z.B. 'de' oder 'en'). Standard: 'de'.
        UnterstÃ¼tzte Werte sind frei â€“ wird direkt an das Modell weitergegeben.
    """

    language_instruction = {
        "de": "Die Ausgabe MUSS auf Deutsch erfolgen.",
        "en": "The output MUST be in English.",
        "fr": "La sortie DOIT Ãªtre en franÃ§ais.",
        "es": "La salida DEBE estar en espaÃ±ol.",
    }.get(language.lower(), f"The output MUST be in language code: {language}")

    return f"""Du bist ein sehr erfahrener {role}. Bitte analysiere das folgende Dokument kritisch aus der Sicht dieser Rolle.

Sprache der Analyse: {language}
{language_instruction}

### Ziel
UnterstÃ¼tze einen Laien (z.B. Patient, Mandant oder Kunde), der dieses Dokument erhalten hat. Gehe dazu auf folgende Punkte ein:

1. **Wichtige Passagen** â€“ besonders relevante Punkte, auf die unbedingt geachtet werden sollte.
2. **Fallstricke** â€“ Formulierungen oder Abschnitte, die sich nachteilig auswirken kÃ¶nnten.
3. **Vorteile** â€“ Inhalte, die vorteilhaft fÃ¼r den Leser sind.
4. **Nachteile** â€“ Inhalte, die zu Nachteilen fÃ¼hren kÃ¶nnten.

### Ausgabeformat
Erstelle eine Markdown-Struktur mit (Ã¼bersetze Ãœberschriften bei anderer Sprache):
- ## Wichtige Punkte / Key Points
- ## Potenzielle Fallstricke / Potential Pitfalls
- ## Vorteile fÃ¼r den Kunden / Advantages
- ## Nachteile fÃ¼r den Kunden / Disadvantages

Falls die gewÃ¤hlte Ausgabesprache nicht Deutsch ist, Ã¼bersetze konsistent alle Ãœberschriften und Inhalte in die Zielsprache.

### Dokumentinhalt
Analysiere den folgenden Text:

{document_text}
"""


def run_ollama(prompt, model="llama3-gradient:8b"):  # alternatives: gemma3, gpt-oss
    try:
        print(f"ğŸ¤– Starte Ollama mit Modell: {model}")
        print(f"ğŸ“ Prompt-LÃ¤nge: {len(prompt)} Zeichen")
        print("â³ Warte auf Antwort von Ollama...")
        print("" + "=" * 50)
        print("ğŸ”„ OLLAMA OUTPUT (Live):")
        print("" + "=" * 50)

        # Start Ollama process with live output
        process = subprocess.Popen(
            ["ollama", "run", model],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,
            universal_newlines=True,
        )

        # Send prompt to process
        if process.stdin:
            process.stdin.write(prompt)
            process.stdin.close()

        output_lines = []
        error_lines = []

        # Read output line by line for real-time display
        while True:
            # Check if process is still running
            poll_result = process.poll()

            # Read available output
            try:
                if process.stdout:
                    line = process.stdout.readline()
                    if line:
                        print(line, end="", flush=True)  # Print in real-time
                        output_lines.append(line)
                    elif poll_result is not None:
                        # Process finished and no more output
                        break
            except Exception:
                if poll_result is not None:
                    break

        # Read any remaining stderr
        if process.stderr:
            stderr_output = process.stderr.read()
            if stderr_output:
                error_lines.append(stderr_output)

        # Wait for process to complete
        return_code = process.wait()

        print("" + "=" * 50)
        print(f"âœ… Ollama beendet (Exit Code: {return_code})")

        if return_code != 0:
            error_msg = "".join(error_lines)
            print(f"âŒ Fehler bei der LLM-AusfÃ¼hrung: {error_msg}")
            sys.exit(1)

        result = "".join(output_lines).strip()
        print(f"ğŸ“Š Antwort-LÃ¤nge: {len(result)} Zeichen")
        return result

    except FileNotFoundError:
        print("âŒ Ollama nicht gefunden! Bitte installiere Ollama erst.")
        print("ğŸ’¡ Installation: brew install ollama")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Unerwarteter Fehler bei der LLM-AusfÃ¼hrung: {e}")
        sys.exit(1)


def save_markdown_output(output_text, output_path):
    try:
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(output_text)
        print(f"\nâœ… Analyse gespeichert unter: {output_path}")
    except Exception as e:
        print(f"Fehler beim Speichern der Datei: {e}")
        sys.exit(1)


def main():
    print("ğŸ“„ PDF Analyse mit lokalem LLM (Ollama)")

    pdf_path_input = input("Gib den Pfad zum PDF-Dokument ein: ").strip()
    role = input(
        "Welche Rolle soll das LLM einnehmen? (z.â€¯B. Jurist, Arzt, Steuerberater): "
    ).strip()
    language = (
        input(
            "In welcher Sprache soll die Analyse ausgegeben werden? (z. B. de, en, fr) [de]: "
        ).strip()
        or "de"
    )
    print(f"ğŸŒ GewÃ¤hlte Ausgabesprache: {language}")

    # Pfad verarbeiten
    print(f"\nğŸ” Verarbeite Pfad: {pdf_path_input}")
    pdf_path = Path(pdf_path_input).expanduser().resolve()
    print(f"ğŸ“ VollstÃ¤ndiger Pfad: {pdf_path}")

    if not pdf_path.exists():
        print(f"âŒ Datei nicht gefunden: {pdf_path}")
        sys.exit(1)

    file_size = pdf_path.stat().st_size
    print(f"ğŸ“Š DateigrÃ¶ÃŸe: {file_size:,} Bytes ({file_size / 1024:.1f} KB)")

    # PDF einlesen
    print("\n" + "=" * 50)
    print("ğŸ” SCHRITT 1: PDF EINLESEN")
    print("" + "=" * 50)
    pdf_text = get_pdf_text(pdf_path)

    if not pdf_text:
        print("âŒ Keine lesbaren Inhalte im PDF gefunden.")
        sys.exit(1)

    # Prompt erstellen
    print("\n" + "=" * 50)
    print("ğŸ§  SCHRITT 2: PROMPT ERSTELLEN")
    print("" + "=" * 50)
    print(f"ğŸ‘¤ GewÃ¤hlte Expertenrolle: {role}")
    print("ğŸ“ Erstelle strukturierten Prompt...")
    prompt = build_prompt(role, pdf_text, language=language)
    print(f"âœ… Prompt erstellt ({len(prompt)} Zeichen, Sprache: {language})")

    # Ollama ausfÃ¼hren
    print("\n" + "=" * 50)
    print("ğŸš€ SCHRITT 3: ANALYSE DURCH OLLAMA")
    print("" + "=" * 50)
    analysis_output = run_ollama(prompt)

    # Ergebnis speichern
    print("\n" + "=" * 50)
    print("ğŸ’¾ SCHRITT 4: ERGEBNIS SPEICHERN")
    print("" + "=" * 50)
    output_txt_path = pdf_path.with_suffix(".analyzed.md.txt")
    print(f"ğŸ“‚ Ziel-Datei: {output_txt_path.name}")
    save_markdown_output(analysis_output, output_txt_path)

    print("\n" + "=" * 50)
    print("ğŸ‰ ANALYSE ABGESCHLOSSEN!")
    print("" + "=" * 50)
    print(f"ğŸ“„ Original: {pdf_path.name}")
    print(f"ğŸ“ Analyse: {output_txt_path.name}")
    print(f"ğŸ‘¤ Rolle: {role}")
    print(f"ğŸ“Š Analyse-LÃ¤nge: {len(analysis_output)} Zeichen")
    print(f"ğŸŒ Sprache: {language}")


if __name__ == "__main__":
    main()
